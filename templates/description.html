<p>"Most traditional benchmarks for computer vision focus on tasks that use a fixed set of labels that are known a priori. On the other hand, tasks like phrase grounding and referring expression comprehension make it possible to probe the
model through natural language, which allows us to gain a more extensive understanding of the modelâ€™s visual understanding capabilities. However, unlike object detection, these free-form text-conditioned box prediction tasks all operate under the assumption that the text corresponds to objects that are necessarily present in the image. Therefore, as shown in the corresponding paper, we demonstrate that results on existing benchmarks tend to overestimate the capabilities of models given they do no necessarily need to understand the context but rather just localize entities." <p>


<p>"This challenge proposes a novel task, Contextual Phrase Detection (CPD) and accompanying dataset called TRICD, that aims to address this blindspot. The dataset consists of instances of two image-text pairs with bounding boxes for each phrase present in the image. The image pairs are contextually related, but partially contradictory: each the images and text pairs are semanitcally similar, but each sentence is only depicted in one of the image and not the other. Crucially, this means there are confirmed negatives in this dataset; for a model to perform well it must refrain from predicting bounding boxes when an object is not present. Thus, to achieve high performance on this challenge, models must demonstrate proficiency at both classification (identifying whether or not an object is present) and localization (finding where a given object is in the image)."</p>
